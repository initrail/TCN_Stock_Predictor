{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Dropout, Dense, Activation, BatchNormalization, Input, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import pandas as pd\n",
    "from pandas_datareader import DataReader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math\n",
    "import seaborn as sns\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['AAPL', 'TSLA', 'NVDA', 'MSFT', 'GOOGL', 'F']\n",
    "start = '10-10-2018'\n",
    "end = '10-10-2020'\n",
    "api = 'yahoo'\n",
    "stocks = {}\n",
    "for symbol in symbols:\n",
    "    stocks[symbol] = DataReader(symbol, api, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in stocks:\n",
    "    data = stocks[symbol]\n",
    "    data['Buy'] = data['Close'] > data['Open']\n",
    "    data['Buy'] = data['Buy'].astype(int)\n",
    "    data['Sell'] = data['Close'] < data['Open']\n",
    "    data['Sell'] = data['Sell'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = 8 #largest dilation factor\n",
    "feature_count = 100 #get 30 unique features\n",
    "final_output = 1 #only one output feature, a number in the range of [0-1] with 1 meaning hold/buy and 0 meaning (don't buy)/sell\n",
    "feature_window = 10 #how big the window is for each feature\n",
    "train_split = feature_window+(feature_window-1)*(ldf-1) #calculate the receptive field of the TCN\n",
    "features = [feature_count]*(int(math.log(ldf)/math.log(2)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = []\n",
    "train_x = []\n",
    "valid_x = []\n",
    "valid_y = []\n",
    "for symbol in stocks:\n",
    "    data = stocks[symbol]\n",
    "    outdata = data[['Buy', 'Sell']].values\n",
    "    \n",
    "    indata = data[['Volume']].copy()\n",
    "    indata['Day Avg'] = data[['Open', 'High', 'Low']].mean(axis=1)\n",
    "    indata = scaler.fit_transform(indata.values)\n",
    "    opendata = indata[:,1]\n",
    "    \n",
    "    train_y.append(outdata[:train_split].reshape((1, train_split, 2)))\n",
    "    valid_y.append(outdata[train_split:2*train_split].reshape((1, train_split, 2)))\n",
    "    \n",
    "    train_x.append(opendata[:train_split].reshape((1, train_split, 1)))\n",
    "    valid_x.append(opendata[train_split:2*train_split].reshape((1, train_split, 1)))\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "valid_x = np.array(valid_x)\n",
    "valid_y = np.array(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1, 73, 2)\n",
      "(6, 1, 73, 2)\n",
      "(6, 1, 73, 1)\n",
      "(6, 1, 73, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "print(valid_y.shape)\n",
    "print(train_x.shape)\n",
    "print(valid_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(Model):\n",
    "    def __init__(self, features, kernel_size, dropout=0.2, activation='relu'):\n",
    "        super(TCN, self).__init__()\n",
    "        self.convs1 = []\n",
    "        self.convs2 = []\n",
    "        self.features = features\n",
    "        self.levels = len(features)\n",
    "        for i in range(self.levels):\n",
    "            self.convs1.append(Conv1D(features[i], kernel_size, 1, 'causal', dilation_rate=2**i))\n",
    "            self.convs2.append(Conv1D(features[i], kernel_size, 1, 'causal', dilation_rate=2**i))\n",
    "        self.maxp = MaxPooling1D(features[0], strides = features[0], padding='same')\n",
    "        self.maxp2 = MaxPooling1D(int(features[0]/2), strides = int(features[0]/2), padding = 'same')\n",
    "        self.batch_norm = BatchNormalization(1)\n",
    "        self.activation = Activation(activation)\n",
    "        self.finalactivation = Activation('softmax')\n",
    "        self.drop = Dropout(dropout, (1, train_split, 1))\n",
    "    def call(self, inputs, training = False):\n",
    "        length = inputs.shape[1]\n",
    "        for i in range(self.levels):\n",
    "            x = self.convs1[i](inputs)\n",
    "            x = tf.reshape(x, (1, length*features[i], 1))\n",
    "            x = self.maxp(x)\n",
    "            x = self.batch_norm(x, training)\n",
    "            x = self.activation(x)\n",
    "            x = self.drop(x, training)\n",
    "\n",
    "            x = self.convs2[i](x)\n",
    "            x = tf.reshape(x, (1, length*features[i], 1))\n",
    "            if i == self.levels-1:\n",
    "                x = self.maxp2(x)\n",
    "                x = tf.reshape(x, (1, train_split, 2))\n",
    "                x = self.batch_norm(x, training)\n",
    "                inputs = self.finalactivation(x + inputs)\n",
    "            else:\n",
    "                x = self.maxp(x)\n",
    "                x = self.batch_norm(x, training)\n",
    "                x = self.activation(x)\n",
    "                x = self.drop(x, training)\n",
    "                inputs = self.activation(x + inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.7853 - categorical_crossentropy: 0.7853 - val_loss: 0.6927 - val_categorical_crossentropy: 0.6927\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7702 - categorical_crossentropy: 0.7702 - val_loss: 0.6933 - val_categorical_crossentropy: 0.6933\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7509 - categorical_crossentropy: 0.7509 - val_loss: 0.6933 - val_categorical_crossentropy: 0.6933\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8112 - categorical_crossentropy: 0.8112 - val_loss: 0.6940 - val_categorical_crossentropy: 0.6940\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7430 - categorical_crossentropy: 0.7430 - val_loss: 0.6939 - val_categorical_crossentropy: 0.6939\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7221 - categorical_crossentropy: 0.7221 - val_loss: 0.6940 - val_categorical_crossentropy: 0.6940\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6890 - categorical_crossentropy: 0.6890 - val_loss: 0.6945 - val_categorical_crossentropy: 0.6945\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7281 - categorical_crossentropy: 0.7281 - val_loss: 0.6947 - val_categorical_crossentropy: 0.6947\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7424 - categorical_crossentropy: 0.7424 - val_loss: 0.6946 - val_categorical_crossentropy: 0.6946\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7180 - categorical_crossentropy: 0.7180 - val_loss: 0.6947 - val_categorical_crossentropy: 0.6947\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6694 - categorical_crossentropy: 0.6694 - val_loss: 0.6946 - val_categorical_crossentropy: 0.6946\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7299 - categorical_crossentropy: 0.7299 - val_loss: 0.6949 - val_categorical_crossentropy: 0.6949\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7066 - categorical_crossentropy: 0.7066 - val_loss: 0.6954 - val_categorical_crossentropy: 0.6954\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7099 - categorical_crossentropy: 0.7099 - val_loss: 0.6950 - val_categorical_crossentropy: 0.6950\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7308 - categorical_crossentropy: 0.7308 - val_loss: 0.6949 - val_categorical_crossentropy: 0.6949\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6783 - categorical_crossentropy: 0.6783 - val_loss: 0.6955 - val_categorical_crossentropy: 0.6955\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7057 - categorical_crossentropy: 0.7057 - val_loss: 0.6960 - val_categorical_crossentropy: 0.6960\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7130 - categorical_crossentropy: 0.7130 - val_loss: 0.6958 - val_categorical_crossentropy: 0.6958\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7413 - categorical_crossentropy: 0.7413 - val_loss: 0.6960 - val_categorical_crossentropy: 0.6960\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6770 - categorical_crossentropy: 0.6770 - val_loss: 0.6960 - val_categorical_crossentropy: 0.6960\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6433 - categorical_crossentropy: 0.6433 - val_loss: 0.6965 - val_categorical_crossentropy: 0.6965\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7254 - categorical_crossentropy: 0.7254 - val_loss: 0.6955 - val_categorical_crossentropy: 0.6955\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7173 - categorical_crossentropy: 0.7173 - val_loss: 0.6957 - val_categorical_crossentropy: 0.6957\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7159 - categorical_crossentropy: 0.7159 - val_loss: 0.6963 - val_categorical_crossentropy: 0.6963\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6891 - categorical_crossentropy: 0.6891 - val_loss: 0.6966 - val_categorical_crossentropy: 0.6966\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6750 - categorical_crossentropy: 0.6750 - val_loss: 0.6958 - val_categorical_crossentropy: 0.6958\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6556 - categorical_crossentropy: 0.6556 - val_loss: 0.6951 - val_categorical_crossentropy: 0.6951\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7068 - categorical_crossentropy: 0.7068 - val_loss: 0.6960 - val_categorical_crossentropy: 0.6960\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7164 - categorical_crossentropy: 0.7164 - val_loss: 0.6977 - val_categorical_crossentropy: 0.6977\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6513 - categorical_crossentropy: 0.6513 - val_loss: 0.6961 - val_categorical_crossentropy: 0.6961\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6786 - categorical_crossentropy: 0.6786 - val_loss: 0.6964 - val_categorical_crossentropy: 0.6964\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7022 - categorical_crossentropy: 0.7022 - val_loss: 0.6969 - val_categorical_crossentropy: 0.6969\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6942 - categorical_crossentropy: 0.6942 - val_loss: 0.6975 - val_categorical_crossentropy: 0.6975\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7304 - categorical_crossentropy: 0.7304 - val_loss: 0.6979 - val_categorical_crossentropy: 0.6979\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6706 - categorical_crossentropy: 0.6706 - val_loss: 0.6981 - val_categorical_crossentropy: 0.6981\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7134 - categorical_crossentropy: 0.7134 - val_loss: 0.6978 - val_categorical_crossentropy: 0.6978\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7288 - categorical_crossentropy: 0.7288 - val_loss: 0.6987 - val_categorical_crossentropy: 0.6987\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7265 - categorical_crossentropy: 0.7265 - val_loss: 0.6996 - val_categorical_crossentropy: 0.6996\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7201 - categorical_crossentropy: 0.7201 - val_loss: 0.7024 - val_categorical_crossentropy: 0.7024\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6744 - categorical_crossentropy: 0.6744 - val_loss: 0.7026 - val_categorical_crossentropy: 0.7026\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7103 - categorical_crossentropy: 0.7103 - val_loss: 0.7009 - val_categorical_crossentropy: 0.7009\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7164 - categorical_crossentropy: 0.7164 - val_loss: 0.7010 - val_categorical_crossentropy: 0.7010\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7361 - categorical_crossentropy: 0.7361 - val_loss: 0.7020 - val_categorical_crossentropy: 0.7020\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6937 - categorical_crossentropy: 0.6937 - val_loss: 0.7025 - val_categorical_crossentropy: 0.7025\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7232 - categorical_crossentropy: 0.7232 - val_loss: 0.7034 - val_categorical_crossentropy: 0.7034\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7402 - categorical_crossentropy: 0.7402 - val_loss: 0.7037 - val_categorical_crossentropy: 0.7037\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7147 - categorical_crossentropy: 0.7147 - val_loss: 0.7016 - val_categorical_crossentropy: 0.7016\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7231 - categorical_crossentropy: 0.7231 - val_loss: 0.7019 - val_categorical_crossentropy: 0.7019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6388 - categorical_crossentropy: 0.6388 - val_loss: 0.7002 - val_categorical_crossentropy: 0.7002\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6853 - categorical_crossentropy: 0.6853 - val_loss: 0.7014 - val_categorical_crossentropy: 0.7014\n"
     ]
    }
   ],
   "source": [
    "model = TCN(features, feature_window, 0.4, 'sigmoid')\n",
    "model.compile(optimizer = 'rmsprop', loss=CategoricalCrossentropy(), metrics=tf.keras.metrics.CategoricalCrossentropy())\n",
    "for i in range(1):\n",
    "    model.fit(x=train_x[i], y=train_y[i], epochs=50, validation_data=(valid_x[i], valid_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]]]\n",
      "[[[0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [1 0]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [0 1]\n",
      "  [1 0]\n",
      "  [1 0]]]\n"
     ]
    }
   ],
   "source": [
    "out = model.predict(train_x[1])\n",
    "for i in range(out.shape[1]):\n",
    "    if out[0, i, 0] > .5:\n",
    "        if i == 0:\n",
    "            print('[[[1 0]')\n",
    "        elif i == out.shape[1] - 1:\n",
    "            print('  [1 0]]]')\n",
    "        else:\n",
    "            print('  [1 0]')\n",
    "    else:\n",
    "        if i == 0:\n",
    "            print('[[[0 1]')\n",
    "        elif i == out.shape[1] -1:\n",
    "            print('  [0 1]]]')\n",
    "        else:\n",
    "            print('  [0 1]')\n",
    "print(train_y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
