{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Temporal Convolutional Network for Daytrading\n",
    "## Daniel Kalam, Sharvita Paithankar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Dropout, Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import pandas as pd\n",
    "from pandas_datareader import DataReader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Getting data for 100 stocks in the date range of April 2nd, 2018 to October 9th, 2020 from yahoo finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "symbols = ['AAPL', 'TSLA', 'FB', 'ROKU']# , 'BRK', 'GOOGL', 'INTC', 'AMD', 'HPE', 'ZM',\n",
    "          #'CAKE', 'AET', 'F', 'KO', 'DDS', 'NVDA', 'NFLX', 'JPM', 'AMZN', 'MSFT']\n",
    "#TODO: Add 80 more symbols.\n",
    "source = 'yahoo'\n",
    "start_date = pd.to_datetime('2019-10-09')\n",
    "end_date = pd.to_datetime('2020-10-09')\n",
    "stock_data_training = {}\n",
    "for symbol in symbols:\n",
    "    stock_data_training[symbol] = DataReader(symbol, source, start_date, end_date)\n",
    "symbols2 = ['NFLX', 'ZM', 'AMZN', 'MSFT']\n",
    "stock_data_validation = {}\n",
    "for symbol in symbols2:\n",
    "    stock_data_validation[symbol] = DataReader(symbol, source, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame for each column in a stock's data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_training_input = np.empty((20, 254, 1))\n",
    "stock_training_output = np.zeros((20, 254, 2))\n",
    "stock_validation_input = np.empty((20, 254, 1))\n",
    "stock_validation_output = np.zeros((20, 254, 2))\n",
    "i = 0\n",
    "scaler = StandardScaler()\n",
    "for symbol in stock_data_training:\n",
    "    close_data = stock_data_training[symbol].Close\n",
    "    open_data = stock_data_training[symbol].Open\n",
    "    #stock_data[symbol].drop(axis= 1, columns = ['Close', 'High', 'Low', 'Volume', 'Adj Close'], inplace = True)\n",
    "    stock_np = stock_data_training[symbol].Open.to_numpy().reshape(254, 1)\n",
    "    stock_np = scaler.fit_transform(stock_np)\n",
    "    #if stock_np.shape == stock_training_data_input[i].shape:\n",
    "    stock_training_input[i, :, :] = stock_np[:, :]\n",
    "    for j in range(0, len(close_data)):\n",
    "        if close_data[j] > open_data[j]:\n",
    "            stock_training_output[i, j, 0] = 1\n",
    "            stock_training_output[i, j, 1] = 0\n",
    "        else:\n",
    "            stock_training_output[i, j, 0] = 0\n",
    "            stock_training_output[i, j, 1] = 1\n",
    "    i+=1\n",
    "for symbol in stock_data_validation:\n",
    "    close_data = stock_data_validation[symbol].Close\n",
    "    open_data = stock_data_validation[symbol].Open\n",
    "    #stock_data[symbol].drop(axis= 1, columns = ['Close', 'High', 'Low', 'Volume', 'Adj Close'], inplace = True)\n",
    "    stock_np = stock_data_validation[symbol].Open.to_numpy().reshape(254, 1)\n",
    "    stock_np = scaler.fit_transform(stock_np)\n",
    "    #if stock_np.shape == stock_training_data_input[i].shape:\n",
    "    stock_validation_input[i, :, :] = stock_np[:, :]\n",
    "    for j in range(0, len(close_data)):\n",
    "        if close_data[j] > open_data[j]:\n",
    "            stock_validation_output[i, j, 0] = 1\n",
    "            stock_validation_output[i, j, 1] = 0\n",
    "        else:\n",
    "            stock_validation_output[i, j, 0] = 0\n",
    "            stock_validation_output[i, j, 1] = 1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "### Converting the Data Into Tensors\n",
    "Turn the data frames into tensorflow datatypes so that they can be processed by tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-376-dbceed9d368c>:7: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  stock_validation_input = np.delete(stock_training_input, delete, axis=0)\n",
      "<ipython-input-376-dbceed9d368c>:8: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  stock_validation_output = np.delete(stock_training_output, delete, axis=0)\n"
     ]
    }
   ],
   "source": [
    "delete = []\n",
    "for j in range(i, 20):\n",
    "    delete.append(j)\n",
    "stock_training_input = np.delete(stock_training_input, delete, axis=0)\n",
    "stock_training_output = np.delete(stock_training_output, delete, axis=0)\n",
    "\n",
    "stock_validation_input = np.delete(stock_training_input, delete, axis=0)\n",
    "stock_validation_output = np.delete(stock_training_output, delete, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_count = 2 # Amount of filters\n",
    "final_filter_count = 2\n",
    "filters = [] # Filter size for each residual block\n",
    "kernel_size = 10 #Resolution of each filter\n",
    "level = kernel_size\n",
    "n = 0\n",
    "while level <= 254:\n",
    "    filters.append(filter_count)\n",
    "    level+=kernel_size + (kernel_size-1)*2**n\n",
    "    n+=1\n",
    "filters[-1] = final_filter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 2)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, dilation_rate, activation,\n",
    "                trainable, dropout, dtype=None, activity_regularizer=None, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(trainable, dtype=dtype)\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.filters = filters\n",
    "        self.adjust_sample = None\n",
    "        self.layer_norm = BatchNormalization(axis=-1)\n",
    "        self.dilatedcausal1 = Conv1D(filters,\n",
    "                                     kernel_size,\n",
    "                                     strides,\n",
    "                                     'causal',\n",
    "                                     dilation_rate=dilation_rate)\n",
    "        self.dilatedcausal2 = Conv1D(filters,\n",
    "                                     kernel_size,\n",
    "                                     strides,\n",
    "                                     'causal',\n",
    "                                     dilation_rate=dilation_rate)\n",
    "\n",
    "    #Make the dropout based on the shape of the input\n",
    "    def build(self, input_shape):\n",
    "        self.drop1 = Dropout(self.dropout, input_shape)\n",
    "        self.drop2 = Dropout(self.dropout, input_shape)\n",
    "        if input_shape[2]!=filters:\n",
    "            self.adjust_sample = Dense(self.filters)\n",
    "\n",
    "    #The residual block processes the input\n",
    "    def call(self, inputs, training):\n",
    "        x = self.dilatedcausal1(inputs)\n",
    "        x = self.layer_norm(x, training)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop1(x, training) #If training is False, drop1 simply returns x\n",
    "        x = self.dilatedcausal2(x)\n",
    "        x = self.layer_norm(x, training)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop2(x, training) #If training is False, drop2 simply returns x\n",
    "        if self.adjust_sample is not None:\n",
    "            inputs = self.adjust_sample(inputs)\n",
    "        return self.activation(x+inputs)\n",
    "        \n",
    "class TCN(Model):\n",
    "    def __init__(self, filters, kernel_size=2, dropout = 0.2, activation='relu',\n",
    "                trainable=False, dtype=None, name=None,\n",
    "                activity_regularizer=None, **kwargs):\n",
    "        super(TCN, self).__init__()\n",
    "        self.levels = []\n",
    "        for i in range(0, len(filters)):\n",
    "            self.levels.append(ResidualBlock(filters[i], kernel_size,\n",
    "                                             1, 2**i, Activation(activation),\n",
    "                                             trainable, dropout,\n",
    "                                             dtype, activity_regularizer))\n",
    "    \n",
    "    #Running the input through each residual block\n",
    "    def call(self, inputs, training=True):\n",
    "        for r_block in self.levels:\n",
    "            inputs = r_block(inputs, training)\n",
    "        return inputs\n",
    "stock_training_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 426ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n"
     ]
    }
   ],
   "source": [
    "tcn_model = TCN(filters, kernel_size, activation='relu', trainable = True, dtype='float')\n",
    "tcn_model.compile(optimizer='adam', loss = 'mse', metrics=['accuracy'])\n",
    "tcn_model.fit(stock_training_input, stock_training_output, epochs = 100, validation_data=(stock_validation_input, stock_validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(tcn_model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
