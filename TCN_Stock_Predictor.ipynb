{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Temporal Convolutional Network for Daytrading\n",
    "## Daniel Kalam, Sharvita Paithankar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Dropout, Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import pandas as pd\n",
    "from pandas_datareader import DataReader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Getting data for 100 stocks in the date range of April 2nd, 2018 to October 9th, 2020 from yahoo finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "symbols = ['AAPL', 'TSLA', 'FB', 'ROKU']# , 'BRK', 'GOOGL', 'INTC', 'AMD', 'HPE', 'ZM',\n",
    "          #'CAKE', 'AET', 'F', 'KO', 'DDS', 'NVDA', 'NFLX', 'JPM', 'AMZN', 'MSFT']\n",
    "#TODO: Add 80 more symbols.\n",
    "source = 'yahoo'\n",
    "start_date = pd.to_datetime('2019-10-09')\n",
    "end_date = pd.to_datetime('2020-10-09')\n",
    "stock_data_training = {}\n",
    "for symbol in symbols:\n",
    "    stock_data_training[symbol] = DataReader(symbol, source, start_date, end_date)\n",
    "symbols2 = ['NFLX', 'ZM', 'AMZN', 'MSFT']\n",
    "stock_data_validation = {}\n",
    "for symbol in symbols2:\n",
    "    stock_data_validation[symbol] = DataReader(symbol, source, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame for each column in a stock's data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_training_input = np.empty((20, 254, 1))\n",
    "stock_training_output = np.zeros((20, 254, 2))\n",
    "stock_validation_input = np.empty((20, 254, 1))\n",
    "stock_validation_output = np.zeros((20, 254, 2))\n",
    "i = 0\n",
    "scaler = StandardScaler()\n",
    "for symbol in stock_data_training:\n",
    "    close_data = stock_data_training[symbol].Close\n",
    "    open_data = stock_data_training[symbol].Open\n",
    "    #stock_data[symbol].drop(axis= 1, columns = ['Close', 'High', 'Low', 'Volume', 'Adj Close'], inplace = True)\n",
    "    stock_np = stock_data_training[symbol].Open.to_numpy().reshape(254, 1)\n",
    "    stock_np = scaler.fit_transform(stock_np)\n",
    "    #if stock_np.shape == stock_training_data_input[i].shape:\n",
    "    stock_training_input[i, :, :] = stock_np[:, :]\n",
    "    for j in range(0, len(close_data)):\n",
    "        if close_data[j] > open_data[j]:\n",
    "            stock_training_output[i, j, 0] = 1\n",
    "            stock_training_output[i, j, 1] = 0\n",
    "        else:\n",
    "            stock_training_output[i, j, 0] = 0\n",
    "            stock_training_output[i, j, 1] = 1\n",
    "    i+=1\n",
    "for symbol in stock_data_validation:\n",
    "    close_data = stock_data_validation[symbol].Close\n",
    "    open_data = stock_data_validation[symbol].Open\n",
    "    #stock_data[symbol].drop(axis= 1, columns = ['Close', 'High', 'Low', 'Volume', 'Adj Close'], inplace = True)\n",
    "    stock_np = stock_data_validation[symbol].Open.to_numpy().reshape(254, 1)\n",
    "    stock_np = scaler.fit_transform(stock_np)\n",
    "    #if stock_np.shape == stock_training_data_input[i].shape:\n",
    "    stock_validation_input[i, :, :] = stock_np[:, :]\n",
    "    for j in range(0, len(close_data)):\n",
    "        if close_data[j] > open_data[j]:\n",
    "            stock_validation_output[i, j, 0] = 1\n",
    "            stock_validation_output[i, j, 1] = 0\n",
    "        else:\n",
    "            stock_validation_output[i, j, 0] = 0\n",
    "            stock_validation_output[i, j, 1] = 1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_data['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close_data['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator = {}\n",
    "# indicator['AAPL'] = {}\n",
    "# for day in range(0, len(open_data['AAPL'])):\n",
    "#     if open_data['AAPL'][day] < close_data['AAPL'][day]:\n",
    "#         indicator['AAPL'][day] = True\n",
    "#     else:\n",
    "#         indicator['AAPL'][day] = False\n",
    "\n",
    "# indicator['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator = {}\n",
    "# indicator['AAPL'] = {}\n",
    "# for day in open_data['AAPL'].keys():\n",
    "# #     print(day)\n",
    "#     if open_data['AAPL'][day] < close_data['AAPL'][day]:\n",
    "#         indicator['AAPL'][str(day)[0:10]] = True\n",
    "#     else:\n",
    "#         indicator['AAPL'][str(day)[0:10]] = False\n",
    "\n",
    "# indicator['AAPL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "### Converting the Data Into Tensors\n",
    "Turn the data frames into tensorflow datatypes so that they can be processed by tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.33190534e+000],\n",
       "        [-1.32030572e+000],\n",
       "        [-1.25560514e+000],\n",
       "        ...,\n",
       "        [ 1.65114978e+000],\n",
       "        [ 1.73518299e+000],\n",
       "        [ 1.68517535e+000]],\n",
       "\n",
       "       [[-1.14746125e+000],\n",
       "        [-1.14088372e+000],\n",
       "        [-1.13777765e+000],\n",
       "        ...,\n",
       "        [ 1.93871407e+000],\n",
       "        [ 2.09293735e+000],\n",
       "        [ 2.02392310e+000]],\n",
       "\n",
       "       [[-1.10250589e+000],\n",
       "        [-1.06787890e+000],\n",
       "        [-1.01325242e+000],\n",
       "        ...,\n",
       "        [ 1.28704689e+000],\n",
       "        [ 1.30316655e+000],\n",
       "        [ 1.44555434e+000]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.00000000e+000],\n",
       "        [ 2.24711642e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        ...,\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307]],\n",
       "\n",
       "       [[ 0.00000000e+000],\n",
       "        [ 0.00000000e+000],\n",
       "        [ 0.00000000e+000],\n",
       "        ...,\n",
       "        [ 2.24711749e+307],\n",
       "        [ 0.00000000e+000],\n",
       "        [ 0.00000000e+000]],\n",
       "\n",
       "       [[ 2.24711642e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        ...,\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307]]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_training_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-187-dbceed9d368c>:7: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  stock_validation_input = np.delete(stock_training_input, delete, axis=0)\n",
      "<ipython-input-187-dbceed9d368c>:8: DeprecationWarning: in the future out of bounds indices will raise an error instead of being ignored by `numpy.delete`.\n",
      "  stock_validation_output = np.delete(stock_training_output, delete, axis=0)\n"
     ]
    }
   ],
   "source": [
    "delete = []\n",
    "for j in range(i, 20):\n",
    "    delete.append(j)\n",
    "stock_training_input = np.delete(stock_training_input, delete, axis=0)\n",
    "stock_training_output = np.delete(stock_training_output, delete, axis=0)\n",
    "\n",
    "stock_validation_input = np.delete(stock_training_input, delete, axis=0)\n",
    "stock_validation_output = np.delete(stock_training_output, delete, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.33190534e+000],\n",
       "        [-1.32030572e+000],\n",
       "        [-1.25560514e+000],\n",
       "        ...,\n",
       "        [ 1.65114978e+000],\n",
       "        [ 1.73518299e+000],\n",
       "        [ 1.68517535e+000]],\n",
       "\n",
       "       [[-1.14746125e+000],\n",
       "        [-1.14088372e+000],\n",
       "        [-1.13777765e+000],\n",
       "        ...,\n",
       "        [ 1.93871407e+000],\n",
       "        [ 2.09293735e+000],\n",
       "        [ 2.02392310e+000]],\n",
       "\n",
       "       [[-1.10250589e+000],\n",
       "        [-1.06787890e+000],\n",
       "        [-1.01325242e+000],\n",
       "        ...,\n",
       "        [ 1.28704689e+000],\n",
       "        [ 1.30316655e+000],\n",
       "        [ 1.44555434e+000]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.00000000e+000],\n",
       "        [ 2.24711642e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        ...,\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307]],\n",
       "\n",
       "       [[ 0.00000000e+000],\n",
       "        [ 0.00000000e+000],\n",
       "        [ 0.00000000e+000],\n",
       "        ...,\n",
       "        [ 2.24711749e+307],\n",
       "        [ 0.00000000e+000],\n",
       "        [ 0.00000000e+000]],\n",
       "\n",
       "       [[ 2.24711642e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        ...,\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307],\n",
       "        [ 2.24711749e+307]]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_training_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_count = 2 # Amount of filters\n",
    "final_filter_count = 2\n",
    "filters = [] # Filter size for each residual block\n",
    "kernel_size = 10 #Resolution of each filter\n",
    "level = kernel_size\n",
    "n = 0\n",
    "while level <= 254:\n",
    "    filters.append(filter_count)\n",
    "    level+=kernel_size + (kernel_size-1)*2**n\n",
    "    n+=1\n",
    "filters[-1] = final_filter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 2)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, dilation_rate, activation,\n",
    "                trainable, dropout, dtype=None, activity_regularizer=None, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(trainable, dtype=dtype)\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.filters = filters\n",
    "        self.adjust_sample = None\n",
    "        self.layer_norm = BatchNormalization(axis=-1)\n",
    "        self.dilatedcausal1 = Conv1D(filters,\n",
    "                                     kernel_size,\n",
    "                                     strides,\n",
    "                                     'causal',\n",
    "                                     dilation_rate=dilation_rate)\n",
    "        self.dilatedcausal2 = Conv1D(filters,\n",
    "                                     kernel_size,\n",
    "                                     strides,\n",
    "                                     'causal',\n",
    "                                     dilation_rate=dilation_rate)\n",
    "\n",
    "    #Make the dropout based on the shape of the input\n",
    "    def build(self, input_shape):\n",
    "        self.drop1 = Dropout(self.dropout, input_shape)\n",
    "        self.drop2 = Dropout(self.dropout, input_shape)\n",
    "        if input_shape[2]!=filters:\n",
    "            self.adjust_sample = Dense(self.filters)\n",
    "\n",
    "    #The residual block processes the input\n",
    "    def call(self, inputs, training):\n",
    "        x = self.dilatedcausal1(inputs)\n",
    "        x = self.layer_norm(x, training)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop1(x, training) #If training is False, drop1 simply returns x\n",
    "        x = self.dilatedcausal2(x)\n",
    "        x = self.layer_norm(x, training)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop2(x, training) #If training is False, drop2 simply returns x\n",
    "        if self.adjust_sample is not None:\n",
    "            inputs = self.adjust_sample(inputs)\n",
    "        return self.activation(x+inputs)\n",
    "        \n",
    "class TCN(Model):\n",
    "    def __init__(self, filters, kernel_size=2, dropout = 0.2, activation='relu',\n",
    "                trainable=False, dtype=None, name=None,\n",
    "                activity_regularizer=None, **kwargs):\n",
    "        super(TCN, self).__init__()\n",
    "        self.levels = []\n",
    "        for i in range(0, len(filters)):\n",
    "            self.levels.append(ResidualBlock(filters[i], kernel_size,\n",
    "                                             1, 2**i, Activation(activation),\n",
    "                                             trainable, dropout,\n",
    "                                             dtype, activity_regularizer))\n",
    "    \n",
    "    #Running the input through each residual block\n",
    "    def call(self, inputs, training=True):\n",
    "        for r_block in self.levels:\n",
    "            inputs = r_block(inputs, training)\n",
    "        return inputs\n",
    "stock_training_output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1/1 [==============================] - 0s 444ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 122/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 182/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 16ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 242/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 14ms/step - loss: nan - accuracy: 0.7707 - val_loss: nan - val_accuracy: 0.7707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x173dd2e9100>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tcn_model = TCN(filters, kernel_size, activation='relu', trainable = True, dtype='float')\n",
    "tcn_model.compile(optimizer='adam', loss = 'mse', metrics=['accuracy'])\n",
    "tcn_model.fit(stock_training_input, stock_training_output, epochs = 250, validation_data=(stock_validation_input, stock_validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x173dd2c1640>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(tcn_model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
