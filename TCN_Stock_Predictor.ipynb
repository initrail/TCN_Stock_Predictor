{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Temporal Convolutional Network for Daytrading\n",
    "## Daniel Kalam, Sharvita Paithankar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Dropout, Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import pandas as pd\n",
    "from pandas_datareader import DataReader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "Getting data for 100 stocks in the date range of April 2nd, 2018 to October 9th, 2020 from yahoo finance."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 25,
>>>>>>> 28bc195401627dceacb9a3e0a6518e8b30c146f8
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "symbols = ['AAPL', 'TSLA', 'FB', 'GE']# , 'BRK', 'GOOGL', 'INTC', 'AMD', 'HPE', 'ZM',\n",
    "          #'CAKE', 'AET', 'F', 'KO', 'DDS', 'NVDA', 'NFLX', 'JPM', 'AMZN', 'MSFT']\n",
    "#TODO: Add 80 more symbols.\n",
    "source = 'yahoo'\n",
    "start_date = pd.to_datetime('2019-10-09')\n",
    "end_date = pd.to_datetime('2020-10-09')\n",
    "stock_data_training = {}\n",
    "for symbol in symbols:\n",
    "    stock_data_training[symbol] = DataReader(symbol, source, start_date, end_date)\n",
    "symbols2 = ['NFLX', 'JPM', 'AMZN', 'MSFT']\n",
    "stock_data_validation = {}\n",
    "for symbol in symbols2:\n",
    "    stock_data_validation[symbol] = DataReader(symbol, source, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame for each column in a stock's data frame."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_training_data_input = np.empty((20, 676, 1))\n",
    "stock_training_data_output = np.zeros((20, 676, 2))\n",
    "i = 0\n",
    "scaler = StandardScaler()\n",
    "for symbol in stock_data:\n",
    "    close_data = stock_data[symbol].Close\n",
    "    open_data = stock_data[symbol].Open\n",
    "    stock_data[symbol].drop(axis= 1, columns = ['Close'], inplace = True)\n",
    "    stock_np = stock_data[symbol].to_numpy()\n",
    "    stock_np = scaler.fit_transform(stock_np)\n",
    "    if stock_np.shape == stock_training_data_input[i].shape:\n",
    "        stock_training_data_input[i, :, 0] = stock_np[:, 2]\n",
    "        for j in range(0, len(close_data[symbol])):\n",
    "            if close_data[j] > open_data[j]:\n",
    "                stock_training_data_output[i, j, 0] = 1\n",
    "                stock_training_data_output[i, j, 1] = 0\n",
    "            else:\n",
    "                stock_training_data_output[i, j, 0] = 0\n",
    "                stock_training_data_output[i, j, 1] = 1\n",
    "        i+=1"
=======
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_training_input = np.empty((20, 254, 1))\n",
    "stock_training_output = np.zeros((20, 254, 2))\n",
    "stock_validation_input = np.empty((20, 254, 1))\n",
    "stock_validation_output = np.zeros((20, 254, 2))\n",
    "i = 0\n",
    "scaler = StandardScaler()\n",
    "for symbol in stock_data_training:\n",
    "    close_data = stock_data_training[symbol].Close\n",
    "    open_data = stock_data_training[symbol].Open\n",
    "    #stock_data[symbol].drop(axis= 1, columns = ['Close', 'High', 'Low', 'Volume', 'Adj Close'], inplace = True)\n",
    "    stock_np = stock_data_training[symbol].Open.to_numpy().reshape(254, 1)\n",
    "    stock_np = scaler.fit_transform(stock_np)\n",
    "    #if stock_np.shape == stock_training_data_input[i].shape:\n",
    "    stock_training_input[i, :, :] = stock_np[:, :]\n",
    "    for j in range(0, len(close_data)):\n",
    "        if close_data[j] > open_data[j]:\n",
    "            stock_training_output[i, j, 0] = 1\n",
    "            stock_training_output[i, j, 1] = 0\n",
    "        else:\n",
    "            stock_training_output[i, j, 0] = 0\n",
    "            stock_training_output[i, j, 1] = 1\n",
    "    i+=1\n",
    "for symbol in stock_data_validation:\n",
    "    close_data = stock_data_validation[symbol].Close\n",
    "    open_data = stock_data_validation[symbol].Open\n",
    "    #stock_data[symbol].drop(axis= 1, columns = ['Close', 'High', 'Low', 'Volume', 'Adj Close'], inplace = True)\n",
    "    stock_np = stock_data_validation[symbol].Open.to_numpy().reshape(254, 1)\n",
    "    stock_np = scaler.fit_transform(stock_np)\n",
    "    #if stock_np.shape == stock_training_data_input[i].shape:\n",
    "    stock_validation_input[i, :, :] = stock_np[:, :]\n",
    "    for j in range(0, len(close_data)):\n",
    "        if close_data[j] > open_data[j]:\n",
    "            stock_validation_output[i, j, 0] = 1\n",
    "            stock_validation_output[i, j, 1] = 0\n",
    "        else:\n",
    "            stock_validation_output[i, j, 0] = 0\n",
    "            stock_validation_output[i, j, 1] = 1\n",
    "    i+=1"
>>>>>>> 28bc195401627dceacb9a3e0a6518e8b30c146f8
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "### Converting the Data Into Tensors\n",
    "Turn the data frames into tensorflow datatypes so that they can be processed by tensorflow."
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete = []\n",
    "for j in range(i, 20):\n",
    "    delete.append(j)\n",
    "stock_training_input = np.delete(stock_training_data_input, delete, axis=0)\n",
    "stock_training_output = np.delete(stock_training_data_output, delete, axis=0)\n",
    "\n",
    "stock_validation_input = np.delete(stock_training_data_input, delete, axis=0)\n",
    "stock_validation_output = np.delete(stock_training_data_output, delete, axis=0)"
   ]
  },
  {
>>>>>>> 28bc195401627dceacb9a3e0a6518e8b30c146f8
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Convolutional Network Class"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 28,
>>>>>>> 28bc195401627dceacb9a3e0a6518e8b30c146f8
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_count = 2 # Amount of filters\n",
    "final_filter_count = 2\n",
    "filters = [] # Filter size for each residual block\n",
    "kernel_size = 10 #Resolution of each filter\n",
    "level = kernel_size\n",
    "n = 0\n",
    "while level <= 254:\n",
    "    filters.append(filter_count)\n",
    "    level+=kernel_size + (kernel_size-1)*2**n\n",
    "    n+=1\n",
    "filters[-1] = final_filter_count"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 29,
>>>>>>> 28bc195401627dceacb9a3e0a6518e8b30c146f8
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, dilation_rate, activation,\n",
    "                trainable, dropout, dtype=None, activity_regularizer=None, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(trainable, dtype=dtype)\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.filters = filters\n",
    "        self.adjust_sample = None\n",
    "        self.layer_norm = BatchNormalization(axis=-1)\n",
    "        self.dilatedcausal1 = Conv1D(filters,\n",
    "                                     kernel_size,\n",
    "                                     strides,\n",
    "                                     'causal',\n",
    "                                     dilation_rate=dilation_rate)\n",
    "        self.dilatedcausal2 = Conv1D(filters,\n",
    "                                     kernel_size,\n",
    "                                     strides,\n",
    "                                     'causal',\n",
    "                                     dilation_rate=dilation_rate)\n",
    "\n",
    "    #Make the dropout based on the shape of the input\n",
    "    def build(self, input_shape):\n",
    "        self.drop1 = Dropout(self.dropout, input_shape)\n",
    "        self.drop2 = Dropout(self.dropout, input_shape)\n",
    "        if input_shape[2]!=filters:\n",
    "            self.adjust_sample = Dense(self.filters)\n",
    "\n",
    "    #The residual block processes the input\n",
    "    def call(self, inputs, training):\n",
    "        x = self.dilatedcausal1(inputs)\n",
    "        x = self.layer_norm(x, training)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop1(x, training) #If training is False, drop1 simply returns x\n",
    "        x = self.dilatedcausal2(x)\n",
    "        x = self.layer_norm(x, training)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop2(x, training) #If training is False, drop2 simply returns x\n",
    "        if self.adjust_sample is not None:\n",
    "            inputs = self.adjust_sample(inputs)\n",
    "        return self.activation(x+inputs)\n",
    "        \n",
    "class TCN(Model):\n",
    "    def __init__(self, filters, kernel_size=2, dropout = 0.2, activation='relu',\n",
    "                trainable=False, dtype=None, name=None,\n",
    "                activity_regularizer=None, **kwargs):\n",
    "        super(TCN, self).__init__()\n",
    "        self.levels = []\n",
    "        for i in range(0, len(filters)):\n",
    "            self.levels.append(ResidualBlock(filters[i], kernel_size,\n",
    "                                             1, 2**i, Activation(activation),\n",
    "                                             trainable, dropout,\n",
    "                                             dtype, activity_regularizer))\n",
    "    \n",
    "    #Running the input through each residual block\n",
    "    def call(self, inputs, training=True):\n",
    "        for r_block in self.levels:\n",
    "            inputs = r_block(inputs, training)\n",
    "        return inputs\n",
    "stock_training_data_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 31,
>>>>>>> 28bc195401627dceacb9a3e0a6518e8b30c146f8
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 2.2489 - accuracy: 0.2982 - val_loss: 0.1642 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.2112 - accuracy: 0.3159 - val_loss: 0.1422 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.2049 - accuracy: 0.3204 - val_loss: 0.1231 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.1575 - accuracy: 0.3041 - val_loss: 0.1058 - val_accuracy: 4.9213e-04\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.0553 - accuracy: 0.3140 - val_loss: 0.0910 - val_accuracy: 9.8425e-04\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.0159 - accuracy: 0.3469 - val_loss: 0.0787 - val_accuracy: 0.0034\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.9394 - accuracy: 0.3278 - val_loss: 0.0682 - val_accuracy: 0.0084\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.8936 - accuracy: 0.3273 - val_loss: 0.0591 - val_accuracy: 0.0108\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.8183 - accuracy: 0.3223 - val_loss: 0.0515 - val_accuracy: 0.0153\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.8536 - accuracy: 0.3337 - val_loss: 0.0452 - val_accuracy: 0.0202\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.7315 - accuracy: 0.3199 - val_loss: 0.0399 - val_accuracy: 0.0276\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.7405 - accuracy: 0.3327 - val_loss: 0.0355 - val_accuracy: 0.0340\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.6550 - accuracy: 0.3509 - val_loss: 0.0318 - val_accuracy: 0.0408\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.5846 - accuracy: 0.3386 - val_loss: 0.0286 - val_accuracy: 0.0472\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.5236 - accuracy: 0.3514 - val_loss: 0.0258 - val_accuracy: 0.0595\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.6386 - accuracy: 0.3583 - val_loss: 0.0235 - val_accuracy: 0.0719\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4535 - accuracy: 0.3376 - val_loss: 0.0215 - val_accuracy: 0.0832\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4524 - accuracy: 0.3499 - val_loss: 0.0198 - val_accuracy: 0.0955\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3984 - accuracy: 0.3386 - val_loss: 0.0184 - val_accuracy: 0.1033\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3961 - accuracy: 0.3450 - val_loss: 0.0171 - val_accuracy: 0.1024\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2691 - accuracy: 0.3223 - val_loss: 0.0159 - val_accuracy: 0.0984\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2589 - accuracy: 0.3268 - val_loss: 0.0148 - val_accuracy: 0.1033\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1958 - accuracy: 0.3479 - val_loss: 0.0138 - val_accuracy: 0.1033\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2550 - accuracy: 0.3622 - val_loss: 0.0129 - val_accuracy: 0.1122\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1461 - accuracy: 0.3425 - val_loss: 0.0121 - val_accuracy: 0.1156\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1326 - accuracy: 0.3647 - val_loss: 0.0114 - val_accuracy: 0.1230\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1071 - accuracy: 0.3563 - val_loss: 0.0107 - val_accuracy: 0.1250\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0375 - accuracy: 0.3415 - val_loss: 0.0101 - val_accuracy: 0.1270\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0486 - accuracy: 0.3543 - val_loss: 0.0096 - val_accuracy: 0.1284\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9991 - accuracy: 0.3287 - val_loss: 0.0092 - val_accuracy: 0.1304\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9104 - accuracy: 0.3465 - val_loss: 0.0088 - val_accuracy: 0.1348\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9169 - accuracy: 0.3740 - val_loss: 0.0085 - val_accuracy: 0.1393\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8705 - accuracy: 0.3573 - val_loss: 0.0082 - val_accuracy: 0.1442\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9254 - accuracy: 0.3612 - val_loss: 0.0079 - val_accuracy: 0.1481\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8533 - accuracy: 0.3711 - val_loss: 0.0077 - val_accuracy: 0.1629\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8546 - accuracy: 0.3602 - val_loss: 0.0075 - val_accuracy: 0.1732\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8305 - accuracy: 0.3922 - val_loss: 0.0073 - val_accuracy: 0.1845\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7690 - accuracy: 0.3706 - val_loss: 0.0071 - val_accuracy: 0.1983\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7594 - accuracy: 0.3691 - val_loss: 0.0070 - val_accuracy: 0.2092\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7114 - accuracy: 0.3765 - val_loss: 0.0069 - val_accuracy: 0.2175\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7268 - accuracy: 0.3794 - val_loss: 0.0068 - val_accuracy: 0.2249\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6784 - accuracy: 0.3637 - val_loss: 0.0067 - val_accuracy: 0.2357\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6392 - accuracy: 0.3735 - val_loss: 0.0066 - val_accuracy: 0.2451\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6443 - accuracy: 0.3819 - val_loss: 0.0066 - val_accuracy: 0.2510\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6552 - accuracy: 0.3937 - val_loss: 0.0065 - val_accuracy: 0.2549\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6386 - accuracy: 0.3780 - val_loss: 0.0065 - val_accuracy: 0.2603\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6024 - accuracy: 0.3927 - val_loss: 0.0064 - val_accuracy: 0.2653\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5712 - accuracy: 0.3917 - val_loss: 0.0063 - val_accuracy: 0.2677\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5911 - accuracy: 0.3706 - val_loss: 0.0063 - val_accuracy: 0.2726\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5687 - accuracy: 0.3927 - val_loss: 0.0062 - val_accuracy: 0.2741\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5452 - accuracy: 0.3784 - val_loss: 0.0062 - val_accuracy: 0.2746\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5152 - accuracy: 0.3686 - val_loss: 0.0061 - val_accuracy: 0.2805\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4987 - accuracy: 0.3839 - val_loss: 0.0061 - val_accuracy: 0.2825\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4889 - accuracy: 0.3873 - val_loss: 0.0060 - val_accuracy: 0.2830\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4698 - accuracy: 0.3912 - val_loss: 0.0059 - val_accuracy: 0.2840\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4456 - accuracy: 0.3907 - val_loss: 0.0058 - val_accuracy: 0.2874\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4557 - accuracy: 0.4409 - val_loss: 0.0058 - val_accuracy: 0.2889\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4466 - accuracy: 0.3848 - val_loss: 0.0057 - val_accuracy: 0.2908\n",
      "Epoch 59/250\n"
     ]
    },
    {
<<<<<<< HEAD
     "ename": "StagingError",
     "evalue": "in user code:\n\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-5-bcebe246852f>:57 call  *\n        inputs = r_block(inputs, training)\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:982 __call__  **\n        self._maybe_build(inputs)\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2643 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    <ipython-input-5-bcebe246852f>:25 build\n        if input_shape[2]!=filters:\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:887 __getitem__\n        return self._dims[key].value\n\n    IndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5b283288aeb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtcn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTCN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mse'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtcn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstock_training_data_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstock_training_data_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStagingError\u001b[0m: in user code:\n\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-5-bcebe246852f>:57 call  *\n        inputs = r_block(inputs, training)\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:982 __call__  **\n        self._maybe_build(inputs)\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2643 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    <ipython-input-5-bcebe246852f>:25 build\n        if input_shape[2]!=filters:\n    C:\\Users\\oneof\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:887 __getitem__\n        return self._dims[key].value\n\n    IndexError: list index out of range\n"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4747 - accuracy: 0.3770 - val_loss: 0.0056 - val_accuracy: 0.2918\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4319 - accuracy: 0.3814 - val_loss: 0.0055 - val_accuracy: 0.2928\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4123 - accuracy: 0.3893 - val_loss: 0.0054 - val_accuracy: 0.2938\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4200 - accuracy: 0.4070 - val_loss: 0.0053 - val_accuracy: 0.2948\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3894 - accuracy: 0.4154 - val_loss: 0.0052 - val_accuracy: 0.2963\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4066 - accuracy: 0.4281 - val_loss: 0.0051 - val_accuracy: 0.2982\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3751 - accuracy: 0.3917 - val_loss: 0.0051 - val_accuracy: 0.2987\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3623 - accuracy: 0.4193 - val_loss: 0.0050 - val_accuracy: 0.3017\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3570 - accuracy: 0.4163 - val_loss: 0.0049 - val_accuracy: 0.3027\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3597 - accuracy: 0.4277 - val_loss: 0.0048 - val_accuracy: 0.3041\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3717 - accuracy: 0.3745 - val_loss: 0.0048 - val_accuracy: 0.3061\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3682 - accuracy: 0.4085 - val_loss: 0.0047 - val_accuracy: 0.3066\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3109 - accuracy: 0.4577 - val_loss: 0.0046 - val_accuracy: 0.3115\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3312 - accuracy: 0.4129 - val_loss: 0.0045 - val_accuracy: 0.3130\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2993 - accuracy: 0.4001 - val_loss: 0.0044 - val_accuracy: 0.3155\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3012 - accuracy: 0.3971 - val_loss: 0.0044 - val_accuracy: 0.3189\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2931 - accuracy: 0.3981 - val_loss: 0.0043 - val_accuracy: 0.3204\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2676 - accuracy: 0.4473 - val_loss: 0.0042 - val_accuracy: 0.3233\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2827 - accuracy: 0.4193 - val_loss: 0.0042 - val_accuracy: 0.4232\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2738 - accuracy: 0.4562 - val_loss: 0.0041 - val_accuracy: 0.4272\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3120 - accuracy: 0.3755 - val_loss: 0.0041 - val_accuracy: 0.4286\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2851 - accuracy: 0.4321 - val_loss: 0.0040 - val_accuracy: 0.4331\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2790 - accuracy: 0.4523 - val_loss: 0.0039 - val_accuracy: 0.4360\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2712 - accuracy: 0.4311 - val_loss: 0.0038 - val_accuracy: 0.4847\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2532 - accuracy: 0.3912 - val_loss: 0.0037 - val_accuracy: 0.4872\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2373 - accuracy: 0.4409 - val_loss: 0.0036 - val_accuracy: 0.4916\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2708 - accuracy: 0.4242 - val_loss: 0.0036 - val_accuracy: 0.4941\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2310 - accuracy: 0.4473 - val_loss: 0.0035 - val_accuracy: 0.4985\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2331 - accuracy: 0.4528 - val_loss: 0.0034 - val_accuracy: 0.5000\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2088 - accuracy: 0.4651 - val_loss: 0.0033 - val_accuracy: 0.5039\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2142 - accuracy: 0.4144 - val_loss: 0.0032 - val_accuracy: 0.5064\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2385 - accuracy: 0.4572 - val_loss: 0.0032 - val_accuracy: 0.5074\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1852 - accuracy: 0.4444 - val_loss: 0.0031 - val_accuracy: 0.5089\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2127 - accuracy: 0.4739 - val_loss: 0.0030 - val_accuracy: 0.5118\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1914 - accuracy: 0.4990 - val_loss: 0.0029 - val_accuracy: 0.5153\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1919 - accuracy: 0.4897 - val_loss: 0.0029 - val_accuracy: 0.5167\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2014 - accuracy: 0.4454 - val_loss: 0.0028 - val_accuracy: 0.5167\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2028 - accuracy: 0.4690 - val_loss: 0.0027 - val_accuracy: 0.5182\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1762 - accuracy: 0.4715 - val_loss: 0.0027 - val_accuracy: 0.5197\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1856 - accuracy: 0.4582 - val_loss: 0.0026 - val_accuracy: 0.5236\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1772 - accuracy: 0.4454 - val_loss: 0.0025 - val_accuracy: 0.5251\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1833 - accuracy: 0.4719 - val_loss: 0.0025 - val_accuracy: 0.5266\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1900 - accuracy: 0.5079 - val_loss: 0.0024 - val_accuracy: 0.5290\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1700 - accuracy: 0.4537 - val_loss: 0.0023 - val_accuracy: 0.5763\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1553 - accuracy: 0.4547 - val_loss: 0.0023 - val_accuracy: 0.5782\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1778 - accuracy: 0.4808 - val_loss: 0.0022 - val_accuracy: 0.5787\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1732 - accuracy: 0.5281 - val_loss: 0.0021 - val_accuracy: 0.5802\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1576 - accuracy: 0.4872 - val_loss: 0.0020 - val_accuracy: 0.5812\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1504 - accuracy: 0.4921 - val_loss: 0.0020 - val_accuracy: 0.5822\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1602 - accuracy: 0.5020 - val_loss: 0.0019 - val_accuracy: 0.5832\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1380 - accuracy: 0.4916 - val_loss: 0.0019 - val_accuracy: 0.5842\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1185 - accuracy: 0.4882 - val_loss: 0.0018 - val_accuracy: 0.5851\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1400 - accuracy: 0.5192 - val_loss: 0.0017 - val_accuracy: 0.5876\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1265 - accuracy: 0.5084 - val_loss: 0.0017 - val_accuracy: 0.5906\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1365 - accuracy: 0.4877 - val_loss: 0.0016 - val_accuracy: 0.5915\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1216 - accuracy: 0.5408 - val_loss: 0.0016 - val_accuracy: 0.6545\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1379 - accuracy: 0.5384 - val_loss: 0.0015 - val_accuracy: 0.6550\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1015 - accuracy: 0.4877 - val_loss: 0.0015 - val_accuracy: 0.6565\n",
      "Epoch 117/250\n"
>>>>>>> 28bc195401627dceacb9a3e0a6518e8b30c146f8
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1055 - accuracy: 0.5374 - val_loss: 0.0014 - val_accuracy: 0.6565\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1260 - accuracy: 0.5123 - val_loss: 0.0014 - val_accuracy: 0.6590\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1007 - accuracy: 0.5034 - val_loss: 0.0013 - val_accuracy: 0.6609\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1265 - accuracy: 0.4951 - val_loss: 0.0013 - val_accuracy: 0.6629\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1200 - accuracy: 0.5502 - val_loss: 0.0013 - val_accuracy: 0.6649\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1098 - accuracy: 0.5207 - val_loss: 0.0012 - val_accuracy: 0.6673\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1096 - accuracy: 0.5118 - val_loss: 0.0012 - val_accuracy: 0.6698\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0967 - accuracy: 0.5477 - val_loss: 0.0011 - val_accuracy: 0.6722\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1110 - accuracy: 0.5714 - val_loss: 0.0011 - val_accuracy: 0.6727\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1053 - accuracy: 0.5536 - val_loss: 0.0010 - val_accuracy: 0.6752\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0984 - accuracy: 0.5723 - val_loss: 0.0010 - val_accuracy: 0.6767\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0872 - accuracy: 0.5379 - val_loss: 9.6218e-04 - val_accuracy: 0.6801\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1059 - accuracy: 0.4951 - val_loss: 9.2881e-04 - val_accuracy: 0.6811\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0873 - accuracy: 0.5281 - val_loss: 8.9318e-04 - val_accuracy: 0.6821\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0749 - accuracy: 0.5266 - val_loss: 8.5765e-04 - val_accuracy: 0.6855\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0924 - accuracy: 0.5773 - val_loss: 8.2933e-04 - val_accuracy: 0.6895\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0827 - accuracy: 0.5492 - val_loss: 8.0009e-04 - val_accuracy: 0.6914\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0805 - accuracy: 0.5123 - val_loss: 7.7064e-04 - val_accuracy: 0.6924\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0798 - accuracy: 0.5773 - val_loss: 7.4025e-04 - val_accuracy: 0.6939\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0849 - accuracy: 0.5586 - val_loss: 7.1385e-04 - val_accuracy: 0.6959\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0723 - accuracy: 0.5074 - val_loss: 6.8657e-04 - val_accuracy: 0.6988\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0865 - accuracy: 0.5108 - val_loss: 6.6153e-04 - val_accuracy: 0.7028\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0798 - accuracy: 0.5261 - val_loss: 6.3115e-04 - val_accuracy: 0.7052\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0665 - accuracy: 0.5177 - val_loss: 6.0259e-04 - val_accuracy: 0.7062\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0822 - accuracy: 0.5832 - val_loss: 5.7660e-04 - val_accuracy: 0.7077\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0720 - accuracy: 0.5600 - val_loss: 5.4978e-04 - val_accuracy: 0.7106\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0718 - accuracy: 0.3533 - val_loss: 5.2399e-04 - val_accuracy: 0.7170\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0648 - accuracy: 0.5950 - val_loss: 4.9797e-04 - val_accuracy: 0.7190\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0580 - accuracy: 0.4065 - val_loss: 4.7377e-04 - val_accuracy: 0.7229\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0597 - accuracy: 0.5192 - val_loss: 4.5127e-04 - val_accuracy: 0.7264\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0688 - accuracy: 0.5837 - val_loss: 4.2808e-04 - val_accuracy: 0.7308\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0593 - accuracy: 0.5655 - val_loss: 4.0744e-04 - val_accuracy: 0.7333\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0655 - accuracy: 0.6014 - val_loss: 3.8773e-04 - val_accuracy: 0.7362\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0662 - accuracy: 0.5605 - val_loss: 3.6831e-04 - val_accuracy: 0.7421\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0582 - accuracy: 0.5910 - val_loss: 3.4928e-04 - val_accuracy: 0.7431\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0663 - accuracy: 0.5418 - val_loss: 3.3180e-04 - val_accuracy: 0.7485\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0659 - accuracy: 0.6009 - val_loss: 3.1522e-04 - val_accuracy: 0.7530\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0561 - accuracy: 0.6280 - val_loss: 3.0143e-04 - val_accuracy: 0.7594\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0624 - accuracy: 0.6196 - val_loss: 2.8761e-04 - val_accuracy: 0.7633\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0522 - accuracy: 0.5925 - val_loss: 2.7188e-04 - val_accuracy: 0.7687\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0663 - accuracy: 0.6211 - val_loss: 2.5833e-04 - val_accuracy: 0.7751\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0495 - accuracy: 0.3819 - val_loss: 2.4564e-04 - val_accuracy: 0.8307\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0520 - accuracy: 0.6014 - val_loss: 2.3480e-04 - val_accuracy: 0.8371\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0533 - accuracy: 0.6009 - val_loss: 2.2441e-04 - val_accuracy: 0.8415\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0543 - accuracy: 0.5689 - val_loss: 2.1382e-04 - val_accuracy: 0.8455\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0488 - accuracy: 0.6353 - val_loss: 2.0516e-04 - val_accuracy: 0.8489\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0487 - accuracy: 0.6132 - val_loss: 1.9701e-04 - val_accuracy: 0.8529\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0387 - accuracy: 0.3376 - val_loss: 1.8930e-04 - val_accuracy: 0.8563\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0425 - accuracy: 0.6284 - val_loss: 1.8140e-04 - val_accuracy: 0.8602\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0456 - accuracy: 0.6048 - val_loss: 1.7430e-04 - val_accuracy: 0.8652\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0476 - accuracy: 0.6127 - val_loss: 1.6722e-04 - val_accuracy: 0.8686\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0451 - accuracy: 0.5920 - val_loss: 1.6063e-04 - val_accuracy: 0.8701\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0430 - accuracy: 0.6117 - val_loss: 1.5454e-04 - val_accuracy: 0.8750\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0560 - accuracy: 0.6058 - val_loss: 1.4869e-04 - val_accuracy: 0.8784\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0375 - accuracy: 0.5969 - val_loss: 1.4351e-04 - val_accuracy: 0.8819\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0346 - accuracy: 0.5979 - val_loss: 1.3830e-04 - val_accuracy: 0.8868\n",
      "Epoch 173/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0406 - accuracy: 0.3789 - val_loss: 1.3323e-04 - val_accuracy: 0.8912\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0347 - accuracy: 0.6412 - val_loss: 1.2766e-04 - val_accuracy: 0.8976\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0442 - accuracy: 0.5738 - val_loss: 1.2235e-04 - val_accuracy: 0.8991\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0465 - accuracy: 0.6117 - val_loss: 1.1711e-04 - val_accuracy: 0.9031\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0340 - accuracy: 0.5837 - val_loss: 1.1182e-04 - val_accuracy: 0.9085\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0392 - accuracy: 0.6206 - val_loss: 1.0726e-04 - val_accuracy: 0.9129\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0379 - accuracy: 0.6073 - val_loss: 1.0241e-04 - val_accuracy: 0.9178\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0377 - accuracy: 0.5950 - val_loss: 9.6683e-05 - val_accuracy: 0.9193\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0351 - accuracy: 0.6132 - val_loss: 9.0953e-05 - val_accuracy: 0.9237\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0420 - accuracy: 0.6299 - val_loss: 8.5799e-05 - val_accuracy: 0.9286\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0289 - accuracy: 0.6471 - val_loss: 8.1068e-05 - val_accuracy: 0.9316\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0332 - accuracy: 0.6580 - val_loss: 7.6633e-05 - val_accuracy: 0.9355\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0347 - accuracy: 0.5994 - val_loss: 7.2715e-05 - val_accuracy: 0.9365\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0291 - accuracy: 0.6314 - val_loss: 6.9274e-05 - val_accuracy: 0.9385\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0289 - accuracy: 0.6373 - val_loss: 6.6297e-05 - val_accuracy: 0.9395\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0338 - accuracy: 0.3632 - val_loss: 6.3199e-05 - val_accuracy: 0.9405\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0337 - accuracy: 0.3878 - val_loss: 6.0335e-05 - val_accuracy: 0.9429\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0308 - accuracy: 0.3844 - val_loss: 5.7698e-05 - val_accuracy: 0.9449\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0255 - accuracy: 0.6511 - val_loss: 5.5172e-05 - val_accuracy: 0.9459\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0270 - accuracy: 0.3637 - val_loss: 5.2913e-05 - val_accuracy: 0.9478\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0346 - accuracy: 0.6240 - val_loss: 5.1026e-05 - val_accuracy: 0.9493\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0379 - accuracy: 0.3794 - val_loss: 4.9062e-05 - val_accuracy: 0.9493\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0312 - accuracy: 0.6378 - val_loss: 4.7300e-05 - val_accuracy: 0.9508\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0281 - accuracy: 0.5940 - val_loss: 4.5633e-05 - val_accuracy: 0.9523\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0287 - accuracy: 0.6580 - val_loss: 4.3863e-05 - val_accuracy: 0.9528\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0232 - accuracy: 0.6860 - val_loss: 4.2064e-05 - val_accuracy: 0.9542\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0261 - accuracy: 0.3445 - val_loss: 4.0412e-05 - val_accuracy: 0.9557\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0225 - accuracy: 0.3189 - val_loss: 3.8973e-05 - val_accuracy: 0.9557\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0218 - accuracy: 0.6422 - val_loss: 3.7649e-05 - val_accuracy: 0.9557\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0269 - accuracy: 0.6535 - val_loss: 3.6026e-05 - val_accuracy: 0.9567\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0258 - accuracy: 0.6176 - val_loss: 3.4459e-05 - val_accuracy: 0.9567\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0230 - accuracy: 0.3681 - val_loss: 3.2876e-05 - val_accuracy: 0.9587\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0199 - accuracy: 0.3440 - val_loss: 3.1493e-05 - val_accuracy: 0.9596\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0264 - accuracy: 0.3627 - val_loss: 3.0253e-05 - val_accuracy: 0.9596\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0199 - accuracy: 0.6683 - val_loss: 2.9008e-05 - val_accuracy: 0.9596\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0212 - accuracy: 0.3863 - val_loss: 2.7740e-05 - val_accuracy: 0.9606\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0214 - accuracy: 0.6555 - val_loss: 2.6428e-05 - val_accuracy: 0.9611\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0209 - accuracy: 0.6550 - val_loss: 2.5286e-05 - val_accuracy: 0.9621\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0269 - accuracy: 0.6801 - val_loss: 2.4251e-05 - val_accuracy: 0.9651\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0172 - accuracy: 0.3543 - val_loss: 2.3028e-05 - val_accuracy: 0.9660\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0204 - accuracy: 0.3794 - val_loss: 2.2014e-05 - val_accuracy: 0.9675\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0232 - accuracy: 0.3947 - val_loss: 2.1097e-05 - val_accuracy: 0.9690\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0259 - accuracy: 0.4149 - val_loss: 2.0358e-05 - val_accuracy: 0.9690\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0173 - accuracy: 0.3612 - val_loss: 1.9665e-05 - val_accuracy: 0.9700\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0189 - accuracy: 0.6575 - val_loss: 1.9055e-05 - val_accuracy: 0.9710\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0202 - accuracy: 0.6334 - val_loss: 1.8446e-05 - val_accuracy: 0.9715\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0142 - accuracy: 0.6693 - val_loss: 1.7818e-05 - val_accuracy: 0.9719\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0237 - accuracy: 0.6191 - val_loss: 1.7087e-05 - val_accuracy: 0.9724\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0201 - accuracy: 0.6373 - val_loss: 1.6346e-05 - val_accuracy: 0.9734\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0186 - accuracy: 0.6698 - val_loss: 1.5695e-05 - val_accuracy: 0.9739\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0161 - accuracy: 0.4094 - val_loss: 1.4928e-05 - val_accuracy: 0.9744\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0157 - accuracy: 0.7633 - val_loss: 1.4202e-05 - val_accuracy: 0.9744\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0202 - accuracy: 0.6693 - val_loss: 1.3538e-05 - val_accuracy: 0.9744\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0196 - accuracy: 0.3686 - val_loss: 1.2986e-05 - val_accuracy: 0.9759\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0161 - accuracy: 0.6634 - val_loss: 1.2482e-05 - val_accuracy: 0.9759\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0153 - accuracy: 0.6796 - val_loss: 1.1974e-05 - val_accuracy: 0.9764\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0179 - accuracy: 0.6575 - val_loss: 1.1513e-05 - val_accuracy: 0.9769\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0167 - accuracy: 0.6841 - val_loss: 1.1122e-05 - val_accuracy: 0.9779\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0141 - accuracy: 0.7776 - val_loss: 1.0771e-05 - val_accuracy: 0.9783\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0147 - accuracy: 0.7835 - val_loss: 1.0375e-05 - val_accuracy: 0.9783\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0132 - accuracy: 0.7003 - val_loss: 1.0038e-05 - val_accuracy: 0.9788\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0194 - accuracy: 0.6245 - val_loss: 9.6416e-06 - val_accuracy: 0.9798\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0168 - accuracy: 0.6294 - val_loss: 9.1992e-06 - val_accuracy: 0.9808\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0158 - accuracy: 0.6850 - val_loss: 8.7052e-06 - val_accuracy: 0.9808\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0153 - accuracy: 0.7023 - val_loss: 8.2448e-06 - val_accuracy: 0.9808\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0189 - accuracy: 0.3583 - val_loss: 7.7813e-06 - val_accuracy: 0.9813\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0132 - accuracy: 0.8100 - val_loss: 7.3624e-06 - val_accuracy: 0.9818\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0175 - accuracy: 0.6772 - val_loss: 7.0373e-06 - val_accuracy: 0.9828\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0125 - accuracy: 0.7958 - val_loss: 6.6537e-06 - val_accuracy: 0.9833\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0138 - accuracy: 0.6944 - val_loss: 6.3308e-06 - val_accuracy: 0.9833\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0141 - accuracy: 0.7495 - val_loss: 6.0172e-06 - val_accuracy: 0.9838\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0141 - accuracy: 0.6875 - val_loss: 5.7093e-06 - val_accuracy: 0.9838\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0134 - accuracy: 0.3597 - val_loss: 5.4436e-06 - val_accuracy: 0.9847\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0146 - accuracy: 0.3376 - val_loss: 5.2009e-06 - val_accuracy: 0.9852\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0130 - accuracy: 0.7908 - val_loss: 4.9448e-06 - val_accuracy: 0.9862\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0132 - accuracy: 0.7741 - val_loss: 4.6975e-06 - val_accuracy: 0.9862\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0106 - accuracy: 0.8091 - val_loss: 4.4693e-06 - val_accuracy: 0.9867\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0202 - accuracy: 0.6900 - val_loss: 4.2558e-06 - val_accuracy: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c61c1e40c8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn_model = TCN(filters, kernel_size, activation='relu', trainable = True, dtype='float')\n",
    "tcn_model.compile(optimizer='adam', loss = 'mse', metrics=['accuracy'])\n",
    "tcn_model.fit(stock_training_input, stock_training_output, epochs = 250, validation_data=(stock_validation_input, stock_validation_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
